---
title: "Practical Machine Learning Course Project"
author: "Priscilla Kurtz"
date: "Thursday, December 18, 2014"
output: html_document
---

This is the course project for the Practical Machine Learning from the Data Science Specialization. The goal of your project is to predict the manner in which they did the exercise.

# Prediction Study Design

First we need to set the seed, load libraries, and data, and do some cleaning. You may need to install parallel package.
```{r}
library(caret)
library(doParallel)
set.seed(12345)

training <- read.csv(file = "pml-training.csv",header = TRUE, sep = ",", na.strings=c("#DIV/0!","<NA>","NA",""," "))
testing <- read.csv(file = "pml-testing.csv", header = TRUE, sep = ",", na.strings=c("#DIV/0!","<NA>","NA",""," "))
```

For cleaning, I remove all columns with only NA values from both training and testing:
```{r}
training_NAs <- apply(training, 2, function(x) {sum(is.na(x))})
training <- training[,which(training_NAs == 0)]

testing_NAs <- apply(testing, 2, function(x) {sum(is.na(x))})
testing <- testing[,which(testing_NAs == 0)]
```

Finishing the cleaning step, a few variables are not related to the class as they hold information about the user performing the exercise or date/time when the execise was taken (e.g. user_name, raw_timestamp_part_1, new_window). We remove those variables.
```{r}
drops <- as.integer(c(1,2,3,4,5,6))
training <- training[,-drops]
testing <- testing[,-drops]
```

# Training

We'll split the training set into two sets for cross validation: 70% for training, and 30% for validation.

```{r}
indexes_for_training <- createDataPartition(y = training$classe, p=0.7, list=FALSE)
training_t <- training[indexes_for_training,]
training_v <- training[-indexes_for_training,]
```

We'll train our model im parallel using cross validation with number of resampling iterations set to 4 and with default parameters except the allowParallel set to true:

```{r}
cl<-makeCluster(3)
registerDoParallel(cl)
trControl <- trainControl(method = "cv", number = 4, allowParallel=TRUE)
modFit <- train(training_t$classe ~ ., method = "rf", trControl = trControl, 
+                      training_t, allowParallel=TRUE)
stopCluster(cl)
modFit
modFit$finalModel
```
Here is the result of the training section:

Random Forest 

13737 samples
   53 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (4 fold) 

Summary of sample sizes: 10303, 10304, 10302, 10302 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD   Kappa SD   
   2    0.9935212  0.9918044  0.0009610583  0.001216669
  27    0.9962876  0.9953039  0.0014133285  0.001788445
  53    0.9945403  0.9930939  0.0019221601  0.002431890

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 

#Validating

We'll validate our model using the predict function with training_v section of our training set:

```{r}
predict <- predict(modFit, training_v)
cm <- confusionMatrix(training_v$classe, predict)
cm
```

The overall accuracy of our model is 0.9966.

Here's the confusion matrix:

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1674    0    0    0    0
         B    5 1133    1    0    0
         C    0    2 1024    0    0
         D    0    0    9  955    0
         E    0    0    0    3 1079

Overall Statistics
                                          
               Accuracy : 0.9966          
                 95% CI : (0.9948, 0.9979)
    No Information Rate : 0.2853          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9957          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9970   0.9982   0.9903   0.9969   1.0000
Specificity            1.0000   0.9987   0.9996   0.9982   0.9994
Pos Pred Value         1.0000   0.9947   0.9981   0.9907   0.9972
Neg Pred Value         0.9988   0.9996   0.9979   0.9994   1.0000
Prevalence             0.2853   0.1929   0.1757   0.1628   0.1833
Detection Rate         0.2845   0.1925   0.1740   0.1623   0.1833
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9985   0.9985   0.9950   0.9975   0.9997

#Predicting

Now, using the testing data set, let's predict using the function that creates the files and upload them to the Coursera Submission part of the assignment:

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

prediction <- predict(modFit, testing)
pml_write_files(prediction)
summary(prediction)
```
All files were given as correct.

#Expectation for in sample and out-of-sample error

The ```modFit$finalModel``` shows a error rate of 0.2%.

I used ```randomForest``` on both data sets of training to get the error rates. The in sample error is rate 0.29% and the out of sample error rate is 0.65% which stands by the idea that the out of sample error is larger than the in sample error.

```{r}
randomForest(classe~., data=training_t)
randomForest(classe~., data=training_v)
```